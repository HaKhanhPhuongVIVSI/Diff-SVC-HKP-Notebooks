{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMvPF8e2lKzyDHgD9o1FGjL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Install Diff-SVC**"],"metadata":{"id":"2Vk9qUu6y01q"}},{"cell_type":"code","source":["#@title #Check GPU type\n","#@markdown At this stage it's not really necessary, the best it does is let you guess how fast it can render\n","!nvidia-smi -L\n","!nvidia-smi"],"metadata":{"cellView":"form","id":"XT6eue7cWc03"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tCd34Lw1u1NE","cellView":"form"},"outputs":[],"source":["#@title #Install Diff-SVC\n","#@markdown The stuff you'll need for every other thing afterwards.\n","#@markdown When you use Diff_SVC_Inference_By_HKP, HKP Will give you the HaKhanhPhuongVIVSI Voice Model in checkpoints folder\n","\n","\n","from IPython.display import clear_output \n","from google.colab import files \n","import os\n","print('Upgrading pip & installing 7zip')\n","!rm -rf /content/sample_data\n","!python -m pip install --upgrade pip\n","!python -m pip install --upgrade wheel\n","!apt-get install unzip\n","!pip install gdown\n","\n","print('Installing torch')\n","%pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n","!pip install --pre torchtext==0.6.0 --no-deps\n","\n","#@markdown ---\n","\n","#@markdown ###Select which repo to use\n","#@markdown The official repo is up-to-date, while UtaUtaUtau's version has Harvest support for improved f0.\n","\n","print('Installing Diff-SVC')\n","repo = 'https://github.com/HaKhanhPhuongVIVSI/diff-svc'#@param [\"https://github.com/prophesier/diff-svc\", \"https://github.com/utautautau/diff-svc\", \"https://github.com/HaKhanhPhuongVIVSI/diff-svc\"]\n","!git clone $repo &> /dev/null\n","%cd \n","\n","print('Installing requirements')\n","%cd \"/content/diff-svc/\"\n","!pip install -r requirements_short.txt\n","!pip install tensorboard<2.9,>=2.8\n","%reload_ext tensorboard\n","\n","%cd \"/content/diff-svc/training/\"\n","!rm config.yaml\n","!gdown 'https://github.com/HaKhanhPhuongVIVSI/Diff-SVC-HKP/releases/download/DiffSVC-HKP/config.yaml' -O config.yaml\n","%cd \"/content/\"\n","!gdown 'https://github.com/HaKhanhPhuongVIVSI/Diff-SVC-HKP/releases/download/DiffSVC-HKP/checkpoints.zip' -O checkpoints.zip\n","%mkdir -p /content/diff-svc/checkpoints/\n","!unzip /content/checkpoints.zip -d /content/diff-svc/\n","\n","print('Done!')"]},{"cell_type":"markdown","source":["# **Mount Google Drive**"],"metadata":{"id":"F3qxa5pOBKk4"}},{"cell_type":"code","source":["#@title Mount Google Drive\n","from google.colab import drive\n","\n","mount_path = '/content/drive/' #@param {type:\"string\"}\n","drive.mount(mount_path)"],"metadata":{"id":"yhPztJTLBTSO","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Inference**"],"metadata":{"id":"H_Vjf9F7ES0g"}},{"cell_type":"code","source":["#@title Load Model\n","\n","#@markdown Load in the full path of your model and config.  \n","\n","#@markdown `project_name` is the name of your singer, `model_path`, as the name states, is the path directory to your model (full path), same goes for `config_path`.\n","\n","#@markdown Example:\n","\n","#@markdown project_name = test\n","\n","#@markdown model_path = `/content/drive/MyDrive/Diff-SVC/checkpoints/test/model_ckpt_steps_50000.ckpt`\n","\n","#@markdown config_path = `/content/drive/MyDrive/Diff-SVC/checkpoints/test/config.yaml`\n","\n","#@markdown ---\n","#@markdown ###**Set model location with the name of the speaker:**\n","#@markdown The model below is a default model, change the settings to use your own model.\n","#@markdown If you wish to use the pre-trained model and don't have your own model, leave these at their default values.\n","\n","#@markdown ---\n","%cd \"/content/diff-svc/\"\n","\n","os.environ['PYTHONPATH']='.'\n","\n","!CUDA_VISIBLE_DEVICES=0\n","from utils.hparams import hparams\n","from preprocessing.data_gen_utils import get_pitch_parselmouth,get_pitch_crepe\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import IPython.display as ipd\n","import utils\n","import librosa\n","import torchcrepe\n","from infer import *\n","import logging\n","from infer_tools.infer_tool import *\n","\n","logging.getLogger('numba').setLevel(logging.WARNING)\n","\n","print('Importted Modules')\n","\n","project_name = 'HaKhanhPhuong'#@param {type:\"string\"}\n","model_path = '/content/diff-svc/checkpoints/model_ckpt_steps_25000.ckpt'#@param {type:\"string\"}\n","config_path = '/content/diff-svc/checkpoints/config.yaml'#@param {type:\"string\"}\n","#@markdown ---\n","\n","#@markdown Disable it if you using CPU version of Diff-SVC Repo\n","hubert_gpu=True#@param {type:\"boolean\"}\n","svc_model = Svc(project_name,config_path,hubert_gpu, model_path)\n","print('model loaded')\n","\n"],"metadata":{"id":"cvwnkmvLEaau","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Upload Audio\n","%cd \"/content/diff-svc/raw/\"\n","\n","print(\"\\n\\033[34m\\033[1mupload your audio\")\n","listfn, length = files.upload().popitem()\n","\n","%cd \"/content/diff-svc/\"\n","print(\"\\n\\033[32m\\033[1mdone\")"],"metadata":{"cellView":"form","id":"6KEElg5K9Rev"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Input Audio and Adjust Parameters\n","\n","\n","wav_fn='/content/diff-svc/raw/test_input.wav'#@param {type:\"string\"} \n","demoaudio, sr = librosa.load(wav_fn)\n","#@markdown ---\n","#@markdown If input audio is male voice and your model is female voice, use this\n","key = 0 #@param {type: \"number\"}\n","\n","#@markdown ---\n","\n","pndm_speedup = 20 #@param {type:\"number\"} \n","wav_gen='test_output.wav'#@param {type:\"string\"} \n","\n","#@markdown ---\n","\n","add_noise_step = 500#@param {type:\"number\"} \n","thre=0.05 #@param {type:\"number\"} \n","\n","#@markdown ---\n","\n","#@markdown use_crepe. It's good but slow, sometimes it's pronouncing is wrong\n","use_crepe=True #@param {type:\"boolean\"}\n","#@markdown ---\n","\n","#@markdown Recommened tick it everytimes\n","use_pe = True #@param {type:\"boolean\"}\n","#@markdown ---\n","\n","#@markdown You get more realistic results with this, because it mixes your voice model and the original singerâ€™s voice.\n","use_gt_mel = False #@param {type:\"boolean\"}\n","\n","f0_tst, f0_pred, audio = run_clip(svc_model,\n","                                  file_path=wav_fn, \n","                                  key=key, \n","                                  acc=pndm_speedup, \n","                                  use_crepe=use_crepe, \n","                                  use_pe=use_pe, \n","                                  thre=0.05,\n","                                  use_gt_mel=use_gt_mel, \n","                                  add_noise_step=add_noise_step,\n","                                  project_name=project_name,\n","                                  out_path=wav_gen)"],"metadata":{"id":"rQ7FRizqItte","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Display Results**"],"metadata":{"id":"IkvflhJcK002"}},{"cell_type":"code","source":["#@title Display Results\n","ipd.display(ipd.Audio(demoaudio, rate=sr))\n","ipd.display(ipd.Audio(audio, rate=hparams['audio_sample_rate'], normalize=False))"],"metadata":{"id":"_V9sN_WEKw57","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Display Graphics**"],"metadata":{"id":"hkSuR6LrLVA8"}},{"cell_type":"code","source":["#@title Display Graphics\n","%matplotlib inline\n","f0_gen,_=get_pitch_parselmouth(*svc_model.vocoder.wav2spec(wav_gen),hparams)\n","f0_tst[f0_tst==0]=np.nan#ground truth f0\n","f0_pred[f0_pred==0]=np.nan#f0 pe predicted\n","f0_gen[f0_gen==0]=np.nan#f0 generated\n","fig=plt.figure(figsize=[15,5])\n","plt.plot(np.arange(0,len(f0_tst)),f0_tst,color='black')\n","plt.plot(np.arange(0,len(f0_pred)),f0_pred,color='orange')\n","plt.plot(np.arange(0,len(f0_gen)),f0_gen,color='red')\n","plt.axhline(librosa.note_to_hz('C4'),ls=\":\",c=\"blue\")\n","plt.axhline(librosa.note_to_hz('G4'),ls=\":\",c=\"green\")\n","plt.axhline(librosa.note_to_hz('C5'),ls=\":\",c=\"orange\")\n","plt.axhline(librosa.note_to_hz('F#5'),ls=\":\",c=\"red\")\n","#plt.axhline(librosa.note_to_hz('A#5'),ls=\":\",c=\"black\")\n","plt.show()"],"metadata":{"id":"8HBaQAFHLeH5","cellView":"form"},"execution_count":null,"outputs":[]}]}